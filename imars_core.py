# imars_core.py (æœ€çµ‚ç‰ˆ - æ”¯æ´å¤šä¾›æ‡‰å•†/å¤š Key å¾ªç’°)

import os
import itertools
from google import genai 
from google.genai import types 
# ç‚ºäº†æ”¯æ´ OpenAI/DeepSeekï¼Œå¼•å…¥å…¶ SDK
try:
    from openai import OpenAI, APIError
except ImportError:
    OpenAI = None
    APIError = Exception 

# --- 1. æµç¨‹æ§åˆ¶å¸¸æ•¸ ---
MAX_ITERATIONS = 4 
TEMPERATURE_INIT = 0.8 
TEMPERATURE_REFINE = 0.4 

# --- 2. ä¾›æ‡‰å•†åŸºç¤é…ç½® ---
VENDOR_CONFIG = {
    # base_url=None è¡¨ç¤ºä½¿ç”¨ SDK é è¨­ç«¯é»
    "gemini": {"base_url": None, "model_prefix": "gemini"},
    "openai": {"base_url": None, "model_prefix": "gpt"},
    "deepseek": {"base_url": "https://api.deepseek.com/v1", "model_prefix": "deepseek"}
    # æ“´å±•æç¤ºï¼šæœªä¾†æ–°å¢ Grokï¼Œå¯ä»¥åœ¨æ­¤è™•æ·»åŠ 
}


# --- 3. ä»£ç†æ¨¡å‹é…ç½® ---

# åˆå§‹è‰ç¨¿ä»£ç† (Agent 0)
agent_initial = {
    'name': "Drafting Agent",
    # ç‚ºæ¯å€‹ä¾›æ‡‰å•†å®šç¾©æ¨¡å‹ï¼Œå¦‚æœä¾›æ‡‰å•†Keyä¸å­˜åœ¨ï¼Œå°‡æœƒå¼•ç™¼éŒ¯èª¤ï¼Œå¼·åˆ¶ç”¨æˆ¶æä¾›æ­£ç¢ºé…ç½®
    'model': {"gemini": "gemini-2.5-flash", "openai": "gpt-3.5-turbo", "deepseek": "deepseek-coder"}, 
    'system_prompt': (
        "ä½ æ˜¯ä¸€å€‹å¿«é€Ÿæ§‹å»ºå°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šç”¨æˆ¶çš„æç¤ºï¼Œç”Ÿæˆä¸€å€‹çµæ§‹æ¸…æ™°ã€å…§å®¹å®Œæ•´çš„åˆå§‹è‰ç¨¿ã€‚ "
        "é‡é»åœ¨æ–¼è¦†è“‹æ‰€æœ‰é—œéµé»ï¼Œè€Œéç´°ç¯€çš„çµ•å°æº–ç¢ºæ€§ã€‚ä½¿ç”¨ Markdown æ ¼å¼ã€‚"
    )
}

# ç²¾ç…‰ä»£ç†æ±  (å¾ªç’°ä½¿ç”¨)
AGENTS = [
    {
        'name': "Logic & Factual Verifier",
        'model': {"gemini": "gemini-2.5-flash", "openai": "gpt-4-turbo", "deepseek": "deepseek-coder"},
        'system_prompt': (
            "ä½ æ˜¯ä¸€å€‹åš´è¬¹çš„é‚è¼¯èˆ‡äº‹å¯¦æ ¸æŸ¥å°ˆå®¶ã€‚ä»”ç´°å¯©æŸ¥æä¾›çš„ç•¶å‰ç­”æ¡ˆï¼Œé‡é»æ‰¾å‡ºé‚è¼¯éŒ¯èª¤ã€çŸ›ç›¾æˆ–éæ™‚çš„äº‹å¯¦ã€‚ "
            "åªå°éŒ¯èª¤å’Œä¸æº–ç¢ºä¹‹è™•é€²è¡Œä¿®æ”¹å’Œä¿®æ­£ï¼Œä¸¦ä¿æŒç­”æ¡ˆåŸæœ‰çš„çµæ§‹ã€‚"
        )
    },
    {
        'name': "Style & Structure Polisher",
        'model': {"gemini": "gemini-2.5-flash", "openai": "gpt-3.5-turbo", "deepseek": "deepseek-chat"},
        'system_prompt': (
            "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡é¢¨å’Œçµæ§‹å„ªåŒ–å¸«ã€‚ä½ çš„ä»»å‹™æ˜¯æå‡ç•¶å‰ç­”æ¡ˆçš„é–±è®€æµæš¢æ€§ã€å°ˆæ¥­åº¦ä»¥åŠæ ¼å¼ç¾è§€åº¦ã€‚ "
            "ç¢ºä¿ä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œã€åˆ—è¡¨å’Œç²—é«”å­—ï¼Œä½¿å…¶æ˜“æ–¼æƒæå’Œé–±è®€ã€‚ä¸è¦æ”¹è®Šæ ¸å¿ƒå…§å®¹ã€‚"
        )
    },
    {
        'name': "Completeness Auditor",
        'model': {"gemini": "gemini-2.5-flash", "openai": "gpt-4o-mini", "deepseek": "deepseek-chat"},
        'system_prompt': (
            "ä½ æ˜¯ä¸€å€‹çŸ¥è­˜å®Œæ•´æ€§å¯©è¨ˆå¸«ã€‚å°æ¯”åŸå§‹å•é¡Œèˆ‡ç•¶å‰ç­”æ¡ˆï¼Œè©•ä¼°æ˜¯å¦éºæ¼äº†ä»»ä½•ç”¨æˆ¶æç¤ºä¸­è¦æ±‚çš„é—œéµè³‡è¨Šæˆ–å­è©±é¡Œã€‚ "
            "å¦‚æœç™¼ç¾éºæ¼ï¼Œè«‹è£œå……å¿…è¦çš„å…§å®¹ä»¥è®“ç­”æ¡ˆæ›´å…¨é¢ï¼Œä¸¦å°‡æ–°å…§å®¹ç„¡ç¸«æ•´åˆåˆ°ç¾æœ‰çµæ§‹ä¸­ã€‚"
        )
    }
]


def call_ai_agent(agent_config, user_prompt, previous_answer, client_info):
    """
    å¯¦éš›å‘¼å« AI API é€²è¡Œè’¸é¤¾èˆ‡ç²¾ç…‰ã€‚
    client_info = {'vendor': 'gemini', 'client': genai.Client, 'key_index': 0}
    """
    vendor = client_info['vendor'].lower()
    client_instance = client_info['client']
    
    if not client_instance:
        raise ConnectionError(f"AI Client ({vendor}) å°šæœªåˆå§‹åŒ–ã€‚")
    
    # 1. æç¤ºæ§‹å»ºèˆ‡æ¨¡å‹è¨­å®š
    model_name = agent_config['model'].get(vendor)
    if not model_name:
         raise KeyError(f"Agent '{agent_config['name']}' æ²’æœ‰ç‚ºä¾›æ‡‰å•† '{vendor}' å®šç¾©æ¨¡å‹ã€‚")

    if agent_config['name'] == "Drafting Agent":
        full_prompt = f"ç”¨æˆ¶åŸå§‹å•é¡Œï¼š\n{user_prompt}\n\n{previous_answer}" 
        temperature = TEMPERATURE_INIT
    else:
        full_prompt = (
            f"ç”¨æˆ¶åŸå§‹å•é¡Œï¼š\n{user_prompt}\n\n"
            f"--- å¾…ç²¾ç…‰çš„ç•¶å‰ç­”æ¡ˆ ---\n{previous_answer}\n\n"
            f"è«‹æ ¹æ“šæ‚¨çš„å°ˆæ¥­è·è²¬ï¼Œä½¿ç”¨ {model_name}ï¼Œç›´æ¥è¼¸å‡ºç²¾ç…‰å¾Œçš„å®Œæ•´ç­”æ¡ˆã€‚"
        )
        temperature = TEMPERATURE_REFINE
    
    system_prompt = agent_config['system_prompt']

    # 2. åŸ·è¡Œ API å‘¼å« (ä¾›æ‡‰å•†åˆ¤æ–·)
    try:
        if vendor == 'gemini':
            # è¨­ç½® Gemini API è«‹æ±‚é…ç½®
            config = types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=temperature
            )
            response = client_instance.models.generate_content(
                model=model_name,
                contents=full_prompt,
                config=config,
            )
            return response.text
        
        elif vendor in ['openai', 'deepseek']:
            # è¨­ç½® OpenAI å…¼å®¹ API è«‹æ±‚åƒæ•¸ (å·²é€éåˆå§‹åŒ–æ™‚çš„ base_url å°å‘æ­£ç¢ºç«¯é»)
            if OpenAI is None:
                 raise NotImplementedError("OpenAI SDK æœªå®‰è£ã€‚è«‹æª¢æŸ¥ requirements.txtã€‚")
                 
            response = client_instance.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_prompt}
                ],
                temperature=temperature
            )
            return response.choices[0].message.content
        
        # Grok ç­‰å…¶ä»–æœå‹™å¯åœ¨æ­¤è™•æ·»åŠ  'elif vendor == "grok":'
        
        else:
            # æ­¤è™•ç†è«–ä¸Šä¸æœƒè¢«åŸ·è¡Œï¼Œå› ç‚ºåœ¨åˆå§‹åŒ–æ™‚å·²ç¶“éæ¿¾
            raise TypeError(f"ä¸æ”¯æ´æˆ–ç„¡æ³•è­˜åˆ¥çš„ AI ä¾›æ‡‰å•†: {vendor}")
            
    except APIError as e: 
        raise RuntimeError(f"API å‘¼å«å¤±æ•— ({vendor}, æ¨¡å‹: {model_name}, Key Index: {client_info['key_index']}): {str(e)}")
    except Exception as e:
        raise RuntimeError(f"API å‘¼å«å¤±æ•— ({vendor}, æ¨¡å‹: {model_name}, Key Index: {client_info['key_index']}): {str(e)}")


def start_imars_refinement(user_prompt, api_keys_pool): 
    """
    ä¸»æ§å‡½æ•¸ï¼šåŸ·è¡Œå¤š Agent è¿­ä»£è’¸é¤¾æµç¨‹ã€‚
    api_keys_pool: [{'vendor': 'gemini', 'key': '...'}, ...]
    """
    if not api_keys_pool:
        error_log = [{'type': 'System', 'title': 'ğŸš¨ åš´é‡éŒ¯èª¤', 'content': 'è«‹æä¾›è‡³å°‘ä¸€å€‹ API Key å’Œä¾›æ‡‰å•†è³‡è¨Šã€‚'}]
        return None, error_log

    process_history = []
    client_pool = []
    
    # 1. åˆå§‹åŒ– Client Pool (åˆå§‹åŒ–æ‰€æœ‰æœ‰æ•ˆçš„å®¢æˆ¶ç«¯)
    for i, config in enumerate(api_keys_pool):
        vendor = config.get('vendor', '').lower()
        api_key = config.get('key')
        client = None
        
        vendor_info = VENDOR_CONFIG.get(vendor)
        if not vendor_info:
            process_history.append({'type': 'Warning', 'title': f'âš ï¸ {vendor} åˆå§‹åŒ–å¤±æ•—', 'content': f'ä¸æ”¯æ´çš„ AI ä¾›æ‡‰å•†ã€‚è·³é Key {i+1}ã€‚'})
            continue
            
        try:
            if vendor == 'gemini':
                client = genai.Client(api_key=api_key)
            
            elif vendor in ['openai', 'deepseek']:
                if OpenAI is not None:
                    # ä½¿ç”¨ base_url ä¾†åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯ (OpenAI base_url ç‚º None)
                    client = OpenAI(
                        api_key=api_key,
                        base_url=vendor_info["base_url"] 
                    )
                else:
                    process_history.append({'type': 'Warning', 'title': f'âš ï¸ {vendor} åˆå§‹åŒ–å¤±æ•—', 'content': 'OpenAI SDK æœªå®‰è£ï¼Œè·³éæ­¤ Keyã€‚'})
                    continue
            
            client_pool.append({
                'vendor': vendor, 
                'client': client,
                'key_index': i + 1 
            })
            process_history.append({'type': 'System', 'title': 'âœ… Client åˆå§‹åŒ–', 'content': f'Key {i+1} (ä¾›æ‡‰å•†: {vendor}) æˆåŠŸåŠ å…¥ Client Poolã€‚'})
            
        except Exception as e:
            process_history.append({'type': 'Error', 'title': f'ğŸš¨ Key {i+1} éŒ¯èª¤', 'content': f'ç„¡æ³•åˆå§‹åŒ– {vendor} Clientã€‚è«‹æª¢æŸ¥å¯†é‘°æ˜¯å¦æœ‰æ•ˆã€‚éŒ¯èª¤: {str(e)}'})

    if not client_pool:
        error_log = [{'type': 'System', 'title': 'ğŸš¨ åš´é‡éŒ¯èª¤', 'content': 'æ‰€æœ‰æä¾›çš„å¯†é‘°éƒ½ç„¡æ•ˆæˆ–ä¾›æ‡‰å•†ä¸è¢«æ”¯æ´ã€‚'}]
        return None, error_log

    # è¨­ç½®å¾ªç’°è¿­ä»£å™¨ï¼šè®“æ¯æ¬¡å‘¼å«éƒ½ä½¿ç”¨ä¸åŒçš„ Key
    client_iterator = itertools.cycle(client_pool)
    
    # 2. åˆå§‹è‰ç¨¿ç”Ÿæˆ (ä½¿ç”¨ç¬¬ä¸€å€‹ Key)
    current_answer = ""
    first_client_info = client_pool[0] 
    initial_instruction = "è«‹æ ¹æ“šåŸå§‹å•é¡Œï¼Œæä¾›ä¸€å€‹çµæ§‹ç°¡å–®çš„åˆå§‹è‰ç¨¿ï¼Œä»¥ä¾¿å¾ŒçºŒ Agent é€²è¡Œç²¾ç…‰ã€‚"
    
    try:
        current_answer = call_ai_agent(
            agent_initial, 
            user_prompt, 
            initial_instruction, 
            first_client_info
        )
        process_history.append({
            'type': 'Agent', 
            'title': f'1. {agent_initial["name"]} (Key {first_client_info["key_index"]} è‰ç¨¿ç”Ÿæˆ)', 
            'content': 'åˆå§‹è‰ç¨¿ç”Ÿæˆå®Œç•¢ã€‚'
        })
    except Exception as e:
        process_history.append({'type': 'Error', 'title': 'ğŸš¨ åˆå§‹è‰ç¨¿ç”Ÿæˆå¤±æ•—', 'content': str(e)})
        return None, process_history
    
    # 3. è¿­ä»£ç²¾ç…‰è¿´åœˆ
    for i in range(MAX_ITERATIONS):
        agent = AGENTS[i % len(AGENTS)] 
        
        # å¾ªç’°å–å‡ºä¸‹ä¸€å€‹ Key
        next_client_info = next(client_iterator)
        
        try:
            refined_answer = call_ai_agent(
                agent, 
                user_prompt, 
                current_answer,
                next_client_info 
            )
            current_answer = refined_answer 
            
            model_used = agent["model"].get(next_client_info["vendor"], "N/A")
            process_history.append({
                'type': 'Agent', 
                'title': f'{i+2}. {agent["name"]} (Key {next_client_info["key_index"]} è¿­ä»£ç²¾ç…‰)', 
                'content': f'æœ¬è¼ªç²¾ç…‰å®Œæˆã€‚ä¾›æ‡‰å•†: {next_client_info["vendor"]}, æ¨¡å‹: {model_used}'
            })
            
        except Exception as e:
            process_history.append({'type': 'Error', 'title': f'ğŸš¨ è¿­ä»£ {i+1} å¤±æ•— ({agent["name"]})', 'content': str(e)})
            break 

    # 4. æœ€çµ‚ç­”æ¡ˆè¿”å›
    return current_answer, process_history
